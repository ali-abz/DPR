
# configuration groups
defaults:
  - encoder: multilingual_bert
  - train: porseman_biencoder
  - datasets: porseman_datasets
  - losses: graded_losses

train_datasets: [porseman_train]
dev_datasets: [porseman_dev]
output_dir: porseman_biencoder_output
train_sampling_rates:
loss_scale_factors:

# Whether to lower case the input text. Set True for uncased models, False for the cased ones.
do_lower_case: True

fix_ctx_encoder: False
val_av_rank_start_epoch: 30
seed: 42
checkpoint_file_name: porseman_biencoder

# A trained bi-encoder checkpoint file to initialize the model
model_file:

# TODO: move to a conf group
# local_rank for distributed training on gpus
local_rank: -1
global_loss_buf_sz: 592000
device:
distributed_world_size:
distributed_port:
no_cuda: False
n_gpu: 1
fp16: False

# For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']."
#        "See details at https://nvidia.github.io/apex/amp.html
fp16_opt_level: O1

# tokens which won't be slit by tokenizer
special_tokens:

ignore_checkpoint_offset: False
ignore_checkpoint_optimizer: False

# set to >1 to enable multiple query encoders
multi_q_encoder: False

# configs related to graded datasets
binary_trainer: False
relations: [1.0, 0.9, 0.8, 0.0, 0.0]

loss_function: RankCosineLoss

