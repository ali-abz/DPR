import torch
import torch.nn.functional as F
from torch import Tensor as T
from IPython import embed


def _NDCG(ranks: T, min_shift=True, verbose=False):
    positions = torch.stack([torch.log2(x) for x in torch.arange(2., len(ranks) + 2)])
    positions = positions.to(ranks.device)
    if min_shift:
        min_value = abs(min(ranks.min().detach(), 0))
        ranks += min_value
    dcg = (ranks / positions).sum()
    sorted_ranks = torch.stack(sorted(ranks, reverse=True))
    ideal_dcg = (sorted_ranks / positions).sum()
    ndcg = dcg / ideal_dcg
    if verbose:
        print(f'NDCG ranks: {ranks}')
        print(f'NDCG ndcg: {ndcg}')
    return ndcg


def ANDCG_loss(scores: T, relations: T, reduction: str = 'mean', version: str = 'v1', min_shift=True, verbose=False, *arg, **kwarg):
    if reduction not in ['mean', 'sum', 'none']:
        error_str = f'reduction is expected to be in ["mean", "sum", "none"], got: {reduction}'
        raise ValueError(error_str)
    if version not in ['v1', 'v2']:
        error_str = f'version is expected to be in ["v1", "v2"], got: {version}'
        raise ValueError(error_str)
    losses = []
    for q_scores, q_relations in zip(scores, relations):
        sorted_q_scores, sorted_q_relations = zip(*sorted(zip(q_scores, q_relations), reverse=True))
        sorted_q_scores = torch.stack(sorted_q_scores)
        sorted_q_relations = torch.stack(sorted_q_relations)
        if verbose:
            print(f'ANDCG sorted_q_scores: {sorted_q_scores}')
            print(f'ANDCG sorted_q_relations: {sorted_q_relations}')
        ndcg = _NDCG(sorted_q_relations * sorted_q_scores, min_shift, verbose)
        if version == 'v1':
            ndcg_loss = 1 - ndcg
        elif version == 'v2':
            eps = 0.00001
            ndcg_loss = (1 / (ndcg + eps)) - (1 / (1 + eps))
        losses.append(ndcg_loss)
    losses = torch.stack(losses)
    if reduction == 'mean':
        return torch.mean(losses)
    if reduction == 'sum':
        return torch.sum(losses)
    return losses


def ramezani_super_simple_loss(scores, relations_per_question, *arg, **kwarg):
    loss = abs(relations_per_question - scores)
    loss = torch.sum(loss)
    return loss


def bce_torch_loss(scores, relations_per_question, reduction='sum', *arg, **kwarg):
    scores = 0.5 * (1 + scores)
    loss = F.binary_cross_entropy(scores, relations_per_question, reduction=reduction)
    return loss


def rank_cosine_loss(scores, relations_per_question, *arg, **kwarg):
    scores = 0.5 * (1 + scores)
    loss = torch.sum(0.5 * (1 - F.cosine_similarity(scores, relations_per_question)))
    return loss


def get_loss_function(loss_parameters):
    name = loss_parameters['name']
    functions = {
        'a3g_loss': ANDCG_loss,
        'ramezani_super_simple_loss': ramezani_super_simple_loss,
        'bce_torch_loss': bce_torch_loss,
        'rank_cosine_loss': rank_cosine_loss,
    }
    loss_function = functions.get(name, None)
    if loss_function is None:
        raise ValueError(f'loss function name "{name}" is not valid.')
    return loss_function
